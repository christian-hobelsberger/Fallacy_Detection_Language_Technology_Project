{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Code adapted based on the [official metrics code of MAFALDA's paper](https://github.com/ChadiHelwe/MAFALDA/blob/main/src/metrics.py)."
      ],
      "metadata": {
        "id": "ATTFrKXrvIax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Span** - the span of a fallacy in a text is the smallest\n",
        "contiguous sequence of sentences that comprises the conclusion and the premises of the fallacy.\n",
        "\n",
        "Evaluation input: fallacy span and label;\n",
        "Evaluation output: F1/Precision/Recall scores"
      ],
      "metadata": {
        "id": "A6h1n4vksgPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "from itertools import chain, combinations\n",
        "from typing import Any, List, Set, Tuple, Union, Dict\n",
        "import json"
      ],
      "metadata": {
        "id": "hnmp8oLqsfPa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform labels to numerical"
      ],
      "metadata": {
        "id": "D8vdgx429v8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL_MAP = {'abusive ad hominem':1,\n",
        "             'ad populum':2,\n",
        "             'appeal to false authority':3,\n",
        "             'appeal to nature':4,\n",
        "             'appeal to tradition':5,\n",
        "             'guilt by association':6,\n",
        "             'tu quoque':7,\n",
        "             'causal oversimplification':8,\n",
        "             'circular reasoning':9,\n",
        "             'equivocation':10,\n",
        "             'false analogy':11,\n",
        "             'false causality':12,\n",
        "             'false dilemma':13,\n",
        "             'hasty generalization':14,\n",
        "             'slippery slope':15,\n",
        "             'straw man':16,\n",
        "             'fallacy of division':17,\n",
        "             'appeal to positive emotion':18,\n",
        "             'appeal to anger':19,\n",
        "             'appeal to fear':20,\n",
        "             'appeal to pity':21,\n",
        "             'appeal to ridicule':22,\n",
        "             'appeal to worse problem':23}"
      ],
      "metadata": {
        "id": "NSRsHZ_h97x5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Labels information"
      ],
      "metadata": {
        "id": "ad122iULuyHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NOTHING_LABEL = 0"
      ],
      "metadata": {
        "id": "w3n2r5WStC4m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of classes on level 2 of the fallacy classification (the deepest one)\n",
        "class NbClasses(Enum):\n",
        "    LVL_2 = 23"
      ],
      "metadata": {
        "id": "4xqhvkrCuqzG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classes for spans"
      ],
      "metadata": {
        "id": "S0-ER8H6uv5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Span:\n",
        "    def __init__(self, span: str):\n",
        "        self.span = span\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f\"{self.span}\"\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return self.__str__()\n",
        "\n",
        "\n",
        "class PredictionSpan(Span):\n",
        "    def __init__(self, span: str, label: Union[int, None], interval: List[int]):\n",
        "        super().__init__(span)\n",
        "        self.label = label\n",
        "        self.interval = interval\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if not isinstance(other, PredictionSpan):\n",
        "            return False\n",
        "        return self.span == other.span\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return super().__str__() + f\" - {self.interval} - {self.label}\"\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return self.__str__()\n",
        "\n",
        "\n",
        "class GroundTruthSpan(Span):\n",
        "    def __init__(self, span: str, labels: Set[Union[int, None]], interval: List[int]):\n",
        "        super().__init__(span)\n",
        "        self.labels = labels\n",
        "        self.interval = interval\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if not isinstance(other, GroundTruthSpan):\n",
        "            return False\n",
        "        return (\n",
        "            self.span == other.span\n",
        "            and self.labels == other.labels\n",
        "            and self.interval == other.interval\n",
        "        )\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return super().__str__() + f\" - {self.interval} - {self.labels}\"\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return self.__str__()\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(\n",
        "            (self.span, tuple(sorted(self.labels)), self.interval[0], self.interval[1])\n",
        "        )"
      ],
      "metadata": {
        "id": "IICAha_6usPw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScoreType(Enum):\n",
        "    PRECISION = 1\n",
        "    RECALL = 2\n",
        "\n",
        "\n",
        "class AnnotatedText:\n",
        "    def __init__(self, spans: List[Union[PredictionSpan, GroundTruthSpan]]):\n",
        "        self.spans = spans\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.spans)\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f\"{self.spans}\"\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return self.__str__()\n",
        "\n",
        "    def generate_label_conjunctions(\n",
        "        self, score_type: ScoreType, to_numeric_labels=False\n",
        "    ) -> List[List[Any]]:\n",
        "        \"\"\"Return a list of all possible conjunctions of spans in the text.\n",
        "        If to_numeric_labels is True, then we keep only the numeric labels of the spans (removing none and nothing values).\n",
        "        Otherwise, returns GroundTruthSpan with one label only.\n",
        "        score_type: PRECISION or RECALL because we don't handle the disjunctions the same way (OR for precision, XOR for recall)\n",
        "        \"\"\"\n",
        "        # Validate and process spans\n",
        "        label_sets = [\n",
        "            s\n",
        "            for s in self.spans\n",
        "            if isinstance(s, GroundTruthSpan) and s.labels and s.labels != {None}\n",
        "        ]\n",
        "        if not all(isinstance(span, GroundTruthSpan) for span in self.spans):\n",
        "            raise Exception(\"This method is only useful for gold spans.\")\n",
        "\n",
        "        def generate_ground_truths(labels, span):\n",
        "            return [\n",
        "                GroundTruthSpan(span.span, {label}, span.interval) for label in labels\n",
        "            ]\n",
        "\n",
        "        def combine_label_sets(label_sets):\n",
        "            if not label_sets:\n",
        "                return [[]]\n",
        "\n",
        "            first_set, rest_sets = label_sets[0], label_sets[1:]\n",
        "            combinations_of_rest = combine_label_sets(rest_sets)\n",
        "\n",
        "            if score_type == ScoreType.PRECISION:\n",
        "                return [\n",
        "                    list(ground_truths) + list(combination)\n",
        "                    for combination in combinations_of_rest\n",
        "                    for ground_truths in chain.from_iterable(\n",
        "                        combinations(\n",
        "                            generate_ground_truths(list(first_set.labels), first_set), r\n",
        "                        )\n",
        "                        for r in range(1, len(first_set.labels) + 1)\n",
        "                    )\n",
        "                ]\n",
        "            else:\n",
        "                return [\n",
        "                    generate_ground_truths([label], first_set) + combination\n",
        "                    for label in first_set.labels\n",
        "                    for combination in combinations_of_rest\n",
        "                ]\n",
        "\n",
        "        all_combinations = combine_label_sets(label_sets)\n",
        "\n",
        "        # Convert combinations to the desired format\n",
        "        def process_combination(combination):\n",
        "            if to_numeric_labels:\n",
        "                return list(\n",
        "                    {\n",
        "                        label\n",
        "                        for span in combination\n",
        "                        for label in span.labels\n",
        "                        if isinstance(label, int) and label > NOTHING_LABEL\n",
        "                    }\n",
        "                )\n",
        "            return [\n",
        "                span\n",
        "                for span in combination\n",
        "                if len(span.labels) == 1 and NOTHING_LABEL not in span.labels\n",
        "            ]\n",
        "\n",
        "        result = [process_combination(combination) for combination in all_combinations]\n",
        "\n",
        "        def are_lists_equal(list1, list2):\n",
        "            # Step 1: Check lengths\n",
        "            if len(list1) != len(list2):\n",
        "                return False\n",
        "\n",
        "            # Step 2: Iterate through elements\n",
        "            for item1, item2 in zip(list1, list2):\n",
        "                # Step 3: Check for equality (customize this part as needed)\n",
        "                if item1 != item2:\n",
        "                    return False\n",
        "\n",
        "            # Step 4: All elements are equal\n",
        "            return True\n",
        "\n",
        "        result = [\n",
        "            lst\n",
        "            for i, lst in enumerate(result)\n",
        "            if not any(\n",
        "                are_lists_equal(lst, lst2) for j, lst2 in enumerate(result) if j < i\n",
        "            )\n",
        "        ]\n",
        "        return result"
      ],
      "metadata": {
        "id": "S_K1Kz-zvBgJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score calculation utilities"
      ],
      "metadata": {
        "id": "iKnMvxj_vFXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def label_score(pred: PredictionSpan, gold: GroundTruthSpan) -> float:\n",
        "    \"\"\"Return 1 if pred values are in gold, 0 else.\n",
        "    Correspond to delta in the C function in the paper\"\"\"\n",
        "    # equivalent to δ(a, b) or F ⊆ F′\n",
        "    # can be change if necessary, to take into account close labels for instance\n",
        "    if pred.label in gold.labels:\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "class PartialScoreType(Enum):\n",
        "    JACCARD_INDEX = 1\n",
        "    PRED_SIZE = 2\n",
        "    GOLD_SIZE = 3\n",
        "\n",
        "\n",
        "def partial_overlap_score(\n",
        "    pred: PredictionSpan,\n",
        "    gold: GroundTruthSpan,\n",
        "    partial_score_type: PartialScoreType = PartialScoreType.JACCARD_INDEX,\n",
        ") -> float:\n",
        "    \"\"\"Return the jaccard index between two spans, or use FINE Grained analysis score.\n",
        "    Corresponds to the fraction in the C function in the paper\"\"\"\n",
        "    # equivalent to Jaccard(a, b) or F ∩ F′ / F ∪ F′\n",
        "    a, b = pred.interval\n",
        "    c, d = gold.interval\n",
        "    # Compute the intersection\n",
        "    intersection_start = max(a, c)\n",
        "    intersection_end = min(b, d)\n",
        "    intersection_length = max(0, intersection_end - intersection_start)\n",
        "\n",
        "    if partial_score_type == PartialScoreType.JACCARD_INDEX:\n",
        "        # Compute the union\n",
        "        union_length = (b - a) + (d - c) - intersection_length\n",
        "\n",
        "        # Check for no overlap\n",
        "        if union_length == 0:\n",
        "            return 0\n",
        "\n",
        "        # Compute the Jaccard index\n",
        "        return intersection_length / union_length\n",
        "    elif partial_score_type == PartialScoreType.PRED_SIZE:\n",
        "        return intersection_length / (b - a) if (b - a) else 0\n",
        "    elif partial_score_type == PartialScoreType.GOLD_SIZE:\n",
        "        return intersection_length / (d - c) if (d - c) else 0\n",
        "\n",
        "\n",
        "def text_full_task_precision(\n",
        "    pred_corpus: AnnotatedText, gold_corpus: AnnotatedText\n",
        ") -> float:\n",
        "    \"\"\"Return the precision score of a prediction (spans + labels).\"\"\"\n",
        "    if not pred_corpus.spans and not gold_corpus.spans:\n",
        "        # there was nothing to predict and nothing was found\n",
        "        return 1\n",
        "    disjunct = gold_corpus.generate_label_conjunctions(ScoreType.PRECISION)\n",
        "    if disjunct == [[]]:\n",
        "        # there was nothing to predict\n",
        "        if any(s.label != NOTHING_LABEL for s in pred_corpus.spans):\n",
        "            # there was something predicted\n",
        "            return 0\n",
        "        # there was nothing predicted\n",
        "        return 1\n",
        "    precision_scores = []\n",
        "    for y_trues in disjunct:\n",
        "        p_sum = []\n",
        "        for pred_span in pred_corpus.spans:\n",
        "            p_sum += [0]\n",
        "            for gold_span in y_trues:\n",
        "                ls = label_score(pred_span, gold_span)\n",
        "                if ls == 0:\n",
        "                    continue\n",
        "                p_pos = partial_overlap_score(\n",
        "                    pred_span, gold_span, PartialScoreType.PRED_SIZE\n",
        "                )\n",
        "                if p_pos * ls > p_sum[-1]:\n",
        "                    p_sum[-1] = p_pos * ls\n",
        "        # print(p_sum)\n",
        "        p_sum = sum(p_sum)\n",
        "\n",
        "        p_denominator = len([s for s in pred_corpus.spans if s.label != NOTHING_LABEL])\n",
        "        if p_denominator == 0:\n",
        "            if len(y_trues) == 0:\n",
        "                precision_scores.append(1)\n",
        "            elif len(y_trues) > 0:\n",
        "                precision_scores.append(0)\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    \"This should not happen: denominator = 0 but len(y_true) > 0\"\n",
        "                )\n",
        "        else:\n",
        "            precision_score = p_sum / p_denominator\n",
        "            if precision_score > 1:\n",
        "                raise Exception(\"This should not happen: precision > 1\")\n",
        "            precision_scores.append(precision_score)\n",
        "    precision = max(precision_scores)\n",
        "    return precision\n",
        "\n",
        "\n",
        "def text_full_task_recall(\n",
        "    pred_corpus: AnnotatedText, gold_corpus: AnnotatedText\n",
        ") -> float:\n",
        "    \"\"\"Return the recall score of a prediction (spans + labels).\"\"\"\n",
        "    if not pred_corpus.spans and not gold_corpus.spans:\n",
        "        # there was nothing to predict and nothing was found\n",
        "        return 1\n",
        "    disjunct = gold_corpus.generate_label_conjunctions(ScoreType.RECALL)\n",
        "    if disjunct == [[]]:\n",
        "        # there was nothing to predict\n",
        "        if any(s.label != NOTHING_LABEL for s in pred_corpus.spans):\n",
        "            # there was something predicted\n",
        "            return 0\n",
        "        # there was nothing predicted\n",
        "        return 1\n",
        "    recall_scores = []\n",
        "    for y_trues in disjunct:\n",
        "        r_sum = []\n",
        "        for gold_span in y_trues:\n",
        "            r_sum += [0]\n",
        "            for pred_span in pred_corpus.spans:\n",
        "                ls = label_score(pred_span, gold_span)\n",
        "                if ls == 0:\n",
        "                    continue\n",
        "                r_pos = partial_overlap_score(\n",
        "                    pred_span, gold_span, PartialScoreType.GOLD_SIZE\n",
        "                )\n",
        "                if r_pos * ls > r_sum[-1]:\n",
        "                    r_sum[-1] = r_pos * ls\n",
        "        r_sum = sum(r_sum)\n",
        "\n",
        "        # we don't count spans wo fallacies:\n",
        "        r_denominator = len(\n",
        "            [s for s in y_trues if None not in s.labels and 0 not in s.labels]\n",
        "        )\n",
        "        if r_denominator == 0:\n",
        "            if len(pred_corpus) == 0:\n",
        "                recall_scores.append(1)\n",
        "            elif len(pred_corpus) > 0:\n",
        "                recall_scores.append(0)\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    \"This should not happen: divide by 0 on recall denominator\"\n",
        "                )\n",
        "        else:\n",
        "            recall_score = r_sum / r_denominator\n",
        "            recall_scores.append(recall_score)\n",
        "    recall = max(recall_scores)\n",
        "    return recall\n",
        "\n",
        "\n",
        "def text_full_task_p_r_f1(\n",
        "    pred_corpus: AnnotatedText, gold_corpus: AnnotatedText\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"Return the precision, recall, and F1 scores of a prediction (spans + labels).\"\"\"\n",
        "    if not pred_corpus.spans and not gold_corpus.spans:\n",
        "        # there was nothing to predict and nothing was found\n",
        "        return 1, 1, 1\n",
        "\n",
        "    precision = text_full_task_precision(pred_corpus, gold_corpus)\n",
        "    recall = text_full_task_recall(pred_corpus, gold_corpus)\n",
        "    f1 = 2 * precision * recall / (precision + recall) if precision + recall else 0\n",
        "    return precision, recall, f1\n",
        "\n",
        "\n",
        "def text_label_only_precision(\n",
        "    pred_text: AnnotatedText, gold_text: AnnotatedText, nb_classes: NbClasses\n",
        ") -> float:\n",
        "    \"\"\"Compute the precision score for labels only.\"\"\"\n",
        "    if not pred_text.spans and not gold_text.spans:\n",
        "        # there was nothing to predict and nothing was found\n",
        "        return 1\n",
        "\n",
        "    y_pred = {\n",
        "        s.label\n",
        "        for s in pred_text.spans\n",
        "        if isinstance(s.label, int)\n",
        "        and s.label != NOTHING_LABEL\n",
        "        and s.label - 1 < nb_classes.value\n",
        "    }\n",
        "    y_trues = [\n",
        "        {\n",
        "            list(s.labels)[0]\n",
        "            for s in y_true\n",
        "            if isinstance(list(s.labels)[0], int)\n",
        "            and list(s.labels)[0] != NOTHING_LABEL\n",
        "            and list(s.labels)[0] - 1 < nb_classes.value\n",
        "        }\n",
        "        for y_true in gold_text.generate_label_conjunctions(ScoreType.PRECISION)\n",
        "    ]\n",
        "    precision_results = []\n",
        "    for y_true in y_trues:\n",
        "        tp = len(set(y_pred).intersection(y_true))\n",
        "        if tp == 0:\n",
        "            if len(y_pred) == 0 and len(y_true) == 0:\n",
        "                precision_results.append(1)\n",
        "            else:\n",
        "                precision_results.append(0)\n",
        "            continue\n",
        "        fp = len(set(y_pred).difference(y_true))\n",
        "        precision_score = tp / (tp + fp)\n",
        "        precision_results.append(precision_score)\n",
        "    precision = max(precision_results)\n",
        "    return precision\n",
        "\n",
        "\n",
        "def text_label_only_recall(\n",
        "    pred_text: AnnotatedText, gold_text: AnnotatedText, nb_classes: NbClasses\n",
        ") -> float:\n",
        "    \"\"\"Compute the recall score for labels only.\"\"\"\n",
        "    if not pred_text.spans and not gold_text.spans:\n",
        "        # there was nothing to predict and nothing was found\n",
        "        return 1\n",
        "\n",
        "    y_pred = {\n",
        "        s.label\n",
        "        for s in pred_text.spans\n",
        "        if isinstance(s.label, int)\n",
        "        and s.label != NOTHING_LABEL\n",
        "        and s.label - 1 < nb_classes.value\n",
        "    }\n",
        "    y_trues = [\n",
        "        {\n",
        "            list(s.labels)[0]\n",
        "            for s in y_true\n",
        "            if isinstance(list(s.labels)[0], int)\n",
        "            and list(s.labels)[0] != NOTHING_LABEL\n",
        "            and list(s.labels)[0] - 1 < nb_classes.value\n",
        "        }\n",
        "        for y_true in gold_text.generate_label_conjunctions(ScoreType.RECALL)\n",
        "    ]\n",
        "    recall_results = []\n",
        "    for y_true in y_trues:\n",
        "        tp = len(set(y_pred).intersection(y_true))\n",
        "        if tp == 0:\n",
        "            if len(y_pred) == 0 and len(y_true) == 0:\n",
        "                recall_results.append(1)\n",
        "            else:\n",
        "                recall_results.append(0)\n",
        "            continue\n",
        "        fn = len(set(y_true).difference(y_pred))\n",
        "        recall_score = tp / (tp + fn)\n",
        "        recall_results.append(recall_score)\n",
        "    recall = max(recall_results)\n",
        "    return recall\n",
        "\n",
        "\n",
        "def text_label_only_p_r_f1(\n",
        "    pred_text: AnnotatedText, gold_text: AnnotatedText, nb_classes: NbClasses\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"Compute the precision, recall, and F1 scores for labels only.\"\"\"\n",
        "    if not pred_text.spans and not gold_text.spans:\n",
        "        # there was nothing to predict and nothing was found\n",
        "        return 1, 1, 1\n",
        "\n",
        "    precision = text_label_only_precision(pred_text, gold_text, nb_classes)\n",
        "    recall = text_label_only_recall(pred_text, gold_text, nb_classes)\n",
        "    f1 = 2 * precision * recall / (precision + recall) if precision + recall else 0\n",
        "    return precision, recall, f1\n"
      ],
      "metadata": {
        "id": "MGqe14DAaLkf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collect everything together for evaluation"
      ],
      "metadata": {
        "id": "TI_Zj-W4tvIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ouput .json file should contain the dictionaries of the following format:\n",
        "\n",
        "```\n",
        "[\n",
        "\n",
        "  {'text 1':[{'fallacy type': [0,1]},{'fallacy type':[2,3]}],\n",
        "  \n",
        "  {'text 2': [{...}, {...}, {...}]},\n",
        "\n",
        "  ...\n",
        "\n",
        "  {'text n': [{...}, {...}]},\n",
        "\n",
        "]\n",
        "```\n",
        "\n",
        "Where [0,1] is a span of a fallacy\n"
      ],
      "metadata": {
        "id": "6lJqNNMb3wNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gold labels are in the following format (initially), the texts are stored separately:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "[[107, 261, 'guilt by association'], [107, 338, 'causal oversimplification'], [158, 338, 'ad populum'], [158, 338, 'nothing'], [391, 542, 'circular reasoning']]\n",
        "```"
      ],
      "metadata": {
        "id": "L7-cPFvx6T_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_predictions(json_data_preds: List[Dict[str, List[Dict[str, List[int]]]]]\n",
        "                        ) -> AnnotatedText:\n",
        "\n",
        "    all_spans = []\n",
        "\n",
        "    for item in json_data_preds:\n",
        "        for text, fallacies in item.items():\n",
        "            for fallacy in fallacies:\n",
        "                for fallacy_type, interval in fallacy.items():\n",
        "                    if fallacy_type.lower() in LABEL_MAP:\n",
        "                        span = PredictionSpan(text, LABEL_MAP[fallacy_type.lower()], interval)\n",
        "                        all_spans.append(span)\n",
        "                    else:\n",
        "                      return f\"No fallacy {fallacy_type} exists in the label set.\"\n",
        "\n",
        "    annotated_text_preds = AnnotatedText(all_spans)\n",
        "    return annotated_text_preds"
      ],
      "metadata": {
        "id": "xTQkRsVt6EiC"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_golds(json_data_golds: List[Dict[str, List[Dict[str, List[int]]]]]\n",
        "                  ) -> AnnotatedText:\n",
        "    all_spans = []\n",
        "\n",
        "    for item in json_data_golds:\n",
        "        for text, fallacies in item.items():\n",
        "            for fallacy in fallacies:\n",
        "                for fallacy_type, interval in fallacy.items():\n",
        "                    if fallacy_type.lower() in LABEL_MAP:\n",
        "                        span = GroundTruthSpan(text, {LABEL_MAP[fallacy_type.lower()]}, interval)\n",
        "                        all_spans.append(span)\n",
        "                    else:\n",
        "                      return f\"No fallacy {fallacy_type} exists in the label set.\"\n",
        "\n",
        "    annotated_text_golds = AnnotatedText(all_spans)\n",
        "    return annotated_text_golds"
      ],
      "metadata": {
        "id": "3gxTsWfmWVfA"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_evaluation(json_predictions_path: str,\n",
        "                   json_golds_path: str,\n",
        "                   labels_only:str=False):\n",
        "\n",
        "    with open(json_predictions_path, 'r') as j:\n",
        "        predictions = json.loads(j.read())\n",
        "\n",
        "    pred_text = collect_predictions(predictions)\n",
        "\n",
        "    with open(json_golds_path, 'r') as j:\n",
        "        golds = json.loads(j.read())\n",
        "\n",
        "    gold_text = collect_golds(golds)\n",
        "\n",
        "    if labels_only:\n",
        "      # Testing text_label_only_p_r_f1: not considering spans\n",
        "      p_l, r_l, f1_l = text_label_only_p_r_f1(\n",
        "          pred_text, gold_text, NbClasses.LVL_2)\n",
        "\n",
        "      print(\"\\n---LABELS ONLY:---\\n\")\n",
        "      print(f\"Precision = {p_l}\")\n",
        "      print(f\"Recall = {r_l}\")\n",
        "      print(f\"F1 score = {f1_l}\")\n",
        "\n",
        "    else:\n",
        "      # Testing text_full_task_p_r_f1: considering spans\n",
        "      p, r, f1 = text_full_task_p_r_f1(pred_text, gold_text)\n",
        "\n",
        "      print(\"\\n---SPANS AND LABELS:---\\n\")\n",
        "      print(f\"Precision = {p}\")\n",
        "      print(f\"Recall = {r}\")\n",
        "      print(f\"F1 score = {f1}\")"
      ],
      "metadata": {
        "id": "EV7rO8Q23f8N"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the evaluation\n",
        "\n",
        "Example structure of example_preds.json file:\n",
        "\n",
        "\n",
        "```\n",
        "[\n",
        "    {\"TITLE: There is a difference between a'smurf' and an'alt'. Please learn it and stop using them interchangeably. POST: Someone once told me they have an 'alt' cause their main account was too high of rank to play with their friends. It's exactly the same as smurfing.\":[{\"false analogy\": [0,12]},{\"Appeal TO Fear\":[12,29]}]},\n",
        "    {\"America is the best place to live, because it's better than any other country.\": [{\"circular reasoning\": [0,78]}]}\n",
        "]\n",
        "```\n",
        "\n",
        "Example structure of example_golds.json file:\n",
        "\n",
        "\n",
        "```\n",
        "[\n",
        "    {\"TITLE: There is a difference between a'smurf' and an'alt'. Please learn it and stop using them interchangeably. POST: Someone once told me they have an 'alt' cause their main account was too high of rank to play with their friends. It's exactly the same as smurfing.\":[{\"appeal to fear\":[12,29]}]},\n",
        "    {\"America is the best place to live, because it's better than any other country.\": [{\"Circular Reasoning\": [0,78]}]}\n",
        "]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "J0r7XjstbZFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_evaluation(json_predictions_path=\"/content/example_preds.json\",\n",
        "               json_golds_path=\"/content/example_golds.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV1pwBn4ZL6v",
        "outputId": "86acc59e-d32c-439d-893f-1799fbb07d9c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---SPANS AND LABELS:---\n",
            "\n",
            "Precision = 0.6666666666666666\n",
            "Recall = 1.0\n",
            "F1 score = 0.8\n"
          ]
        }
      ]
    }
  ]
}