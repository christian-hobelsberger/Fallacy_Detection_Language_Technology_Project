{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy0vG_pmOY_4"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NBDgf_pIfZS"
      },
      "outputs": [],
      "source": [
        "!pip install -U \"transformers==4.40.0\" --upgrade\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhjKL5R6yzYn"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx8LzAiDOhy7"
      },
      "source": [
        "## Inialize the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLmtbescy7ez"
      },
      "source": [
        "Llama 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6rEsdO_IxPV"
      },
      "outputs": [],
      "source": [
        "model_id_llama3 = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
        "\n",
        "llama3 = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id_llama3,\n",
        "    model_kwargs={\n",
        "        \"torch_dtype\": torch.float16,\n",
        "        \"quantization_config\": {\"load_in_4bit\": True},\n",
        "        \"low_cpu_mem_usage\": True,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl2oI8IHyiZW"
      },
      "source": [
        "Gemma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVyH42guyoRE"
      },
      "outputs": [],
      "source": [
        "model_id_gemma = \"google/gemma-7b-it\"\n",
        "\n",
        "gemma = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id_gemma,\n",
        "    model_kwargs={\n",
        "        \"torch_dtype\": torch.float16,\n",
        "        \"quantization_config\": {\"load_in_4bit\": True},\n",
        "        \"low_cpu_mem_usage\": True,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_id_mistral = \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\"\n",
        "\n",
        "mistral = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id_mistral,\n",
        "    model_kwargs={\n",
        "        \"torch_dtype\": torch.float16,\n",
        "        \"quantization_config\": {\"load_in_4bit\": True},\n",
        "        \"low_cpu_mem_usage\": True,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnysTccigPk2"
      },
      "source": [
        "## Fallacies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyRd3ns8wreR"
      },
      "outputs": [],
      "source": [
        "fallacies = \"\"\"\n",
        "Fallacy of Credibility: Abusive Ad Hominem; Ad Populum; Appeal to False Authority;\n",
        "Appeal to Nature; Appeal to Tradition; Guilt by Association; Tu Quoque.\n",
        "Fallacy of Logic: Causal Oversimplification; Circular Reasoning; Equivocation;\n",
        "False Analogy; False Causality; False Dilemma; Hasty Generalization; Slippery Slope; Straw Man; Fallacy of Division.\n",
        "Appeal to Emotion: Appeal to Positive Emotion; Appeal to Anger; Appeal to Fear;\n",
        "Appeal to Pity; Appeal to Ridicule; Appeal to Worse Problem.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrTIE9PU4TM4"
      },
      "outputs": [],
      "source": [
        "fallacies_with_examples = \"\"\"\n",
        "Slippery slope:\\n\\nIf we ban assault rifles, next they'll ban all guns, and before you know it, we'll be living in a police state.,\n",
        "Hasty generalization:\\n\\nI met two people from New York City, and both were rude. Therefore, all New Yorkers are rude.,\n",
        "False analogy:\\n\\nComparing buying a car to choosing a spouse is a false analogy because the two situations are vastly different.,\n",
        "Guilt by association:\\n\\nYou shouldn't listen to John's opinion on politics; he's friends with several criminals.,\n",
        "Causal oversimplification:\\n\\nThe rise in crime rates is due to violent video games; therefore, banning them will solve the problem.,\n",
        "Ad populum:\\n\\nEveryone is buying the new smartphone, so it must be the best one on the market.,\n",
        "Circular reasoning:\\n\\nThe Bible is the word of God because God says it is in the Bible.,\n",
        "Appeal to fear:\\n\\nIf you don't vote for me, your taxes will skyrocket and crime will run rampant.,\n",
        "False causality:\\n\\nEvery time I wash my car, it rains. Therefore, washing my car causes rain.,\n",
        "Fallacy of division:\\n\\nThat company is profitable, so all of its employees must be well-paid.,\n",
        "Appeal to ridicule:\\n\\nYou shouldn't listen to Mary's argument about climate change; she believes the Earth is flat!,\n",
        "Appeal to worse problems:\\n\\nWhy are we worrying about pollution when there are children starving in the world?,\n",
        "Abusive ad hominem:\\n\\nYou can't trust anything Tom says; he's just a loser.,\n",
        "Appeal to false authority:\\n\\nMy doctor says that smoking is good for you, so it must be true.,\n",
        "Appeal to positive emotion:\\n\\nIf you care about the environment, you'll buy this eco-friendly product.,\n",
        "Appeal to nature:\\n\\nNatural remedies are always better than pharmaceutical drugs.,\n",
        "Equivocation:\\n\\nThe sign said 'Fine for parking here,' so I thought it would be fine to park there.,\n",
        "Appeal to anger:\\n\\nYou wouldn't want to be a coward, would you? Then you better join the military!,\n",
        "False dilemma:\\n\\nEither we ban all guns or we'll continue to have mass shootings.,\n",
        "Straw man:\\n\\nOpponents of gun control argue that we should give guns to everyone, including criminals and children.,\n",
        "Appeal to tradition:\\n\\nWe should continue to use fossil fuels because that's how we've always done it.,\n",
        "Tu quoque:\\n\\nYou say I shouldn't smoke, but I've seen you smoke before!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EhdDNhdvGFO"
      },
      "source": [
        "### Version â„–1: Vanilla zero-shot version of ToT prompting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWeoOFzAbf2h"
      },
      "outputs": [],
      "source": [
        "def tot_prompt_1(input_text, pipeline):\n",
        "    messages = [\n",
        "        {\"role\"   : \"user\",\n",
        "        \"content\":\n",
        "        f\"As a content reviewer, I provide multiple fallacies which might occur in the text; \\\n",
        "        you need to identify the falacies if there're such in the text.\\\n",
        "        {fallacies}\\\n",
        "        Text: {input_text}\\\n",
        "        Walk me through this context in manageable parts step by step, \\\n",
        "        summarizing and analyzing as we go.\\\n",
        "        Answer:\\\n",
        "        \"},\n",
        "    ]\n",
        "\n",
        "    prompt = pipeline.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    terminators = [\n",
        "        pipeline.tokenizer.eos_token_id,\n",
        "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    outputs = pipeline(\n",
        "        prompt,\n",
        "        max_new_tokens=512,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "    return outputs[0][\"generated_text\"][len(prompt):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbMq2xyGvBW4",
        "outputId": "5a6b1b80-6b8b-4b36-9524-e485291ddbed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's break down the text and identify any fallacies that might be present.\n",
            "\n",
            "**Step 1: Understand the context**\n",
            "The author is discussing the difference between an \"smurf\" and an \"alt\" in the context of online gaming.\n",
            "\n",
            "**Step 2: Identify the claims**\n",
            "The author makes two claims:\n",
            "\n",
            "1. \"Someone once told me they have an'alt' cause their main account was too high of rank to play with their friends.\"\n",
            "2. \"It's exactly the same as smurfing.\"\n",
            "\n",
            "**Step 3: Analyze the claims**\n",
            "\n",
            "1. The first claim is an anecdote, which is a personal experience or story told by someone. However, the author doesn't provide any further context or evidence to support this claim. This could be an example of the **Appeal to Authority** fallacy, where the author is relying on an unverified anecdote as evidence.\n",
            "2. The second claim is a comparison between having an \"alt\" and \"smurfing\". The author is implying that having an \"alt\" is equivalent to \"smurfing\", which is a common gaming term for creating a new account to gain an advantage. However, the author doesn't provide any logical or evidence-based reasoning to support this comparison. This could be an example of the **False Analogy** fallacy, where the author is drawing an unjustified comparison between two things.\n",
            "\n",
            "**Step 4: Identify any fallacies**\n",
            "\n",
            "Based on the analysis, we can identify the following fallacies:\n",
            "\n",
            "1. **Appeal to Authority** (or lack thereof): The author relies on an unverified anecdote as evidence, without providing any further context or evidence to support their claim.\n",
            "2. **False Analogy**: The author draws an unjustified comparison between having an \"alt\" and \"smurfing\", without providing any logical or evidence-based reasoning to support this comparison.\n",
            "\n",
            "**Step 5: Summarize and analyze the text**\n",
            "\n",
            "In summary, the text appears to be a short discussion about the difference between an \"smurf\" and an \"alt\" in the context of online gaming. However, the author's claims are based on an unverified anecdote and an unjustified comparison, which could be examples of fallacious reasoning. To strengthen their argument, the author could provide more evidence or logical reasoning to support their claims.\n"
          ]
        }
      ],
      "source": [
        "# The answer is \"false analogy\"\n",
        "print(tot_prompt_1(input_text=\n",
        "             \"\"\"\n",
        "             TITLE: There is a difference between a'smurf' and an'alt'.\n",
        "             Please learn it and stop using them interchangeably. POST: Someone once told me\n",
        "             they have an'alt' cause their main account was too high of rank to play with their\n",
        "             friends. It's exactly the same as smurfing.\n",
        "             \"\"\",\n",
        "                   pipeline=llama3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8u3Srwi8t2K",
        "outputId": "27d91228-e708-4370-fa3f-68c55d959d9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Fallacies in the text:**\n",
            "\n",
            "**1. Ad Populum:** The statement \"America is the best place to live\" is an ad populum fallacy, as it relies on the popularity of the United States to support the claim.\n",
            "\n",
            "**2. Appeal to Tradition:** The statement \"America is the best place to live\" also appeals to tradition, as it is a commonly held belief in the United States.\n",
            "\n",
            "**3. Straw Man:** The statement \"America is the best place to live\" does not address the potential drawbacks of living in the United States, such as high crime rates or income inequality.\n",
            "\n",
            "**Answer:**\n",
            "\n",
            "The text claims that America is the best place to live, based on the premise that it is better than any other country. However, this claim is fallacious due to the ad populum, appeal to tradition, and straw man fallacies discussed above. There are many factors to consider when comparing countries, and it is important to weigh the pros and cons of each place before making a judgment.\n"
          ]
        }
      ],
      "source": [
        "# The answer is \"circular reasoning\"\n",
        "print(tot_prompt_1(input_text=\n",
        "             \"\"\"\n",
        "             America is the best place to live, because it's better than any other country.\n",
        "             \"\"\",\n",
        "                   pipeline=gemma))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTNZwTTWwUAr",
        "outputId": "a1ba38ac-a948-48e4-d1ad-47531e4aa978"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's break down the text and identify any potential fallacies.\n",
            "\n",
            "**Step 1: Identify the claims**\n",
            "\n",
            "The text consists of two claims:\n",
            "\n",
            "1. \"The last Democrat winner of the New Hampshire primary won the general election.\"\n",
            "2. \"This year, the winner of the New Hampshire primary will win the general election.\"\n",
            "\n",
            "**Step 2: Analyze the first claim**\n",
            "\n",
            "The first claim is a statement of fact. It appears to be a historical fact about a previous election. There is no apparent fallacy in this claim.\n",
            "\n",
            "**Step 3: Analyze the second claim**\n",
            "\n",
            "The second claim is a prediction. It is making a claim about the outcome of an upcoming election. Here, we can identify a potential fallacy:\n",
            "\n",
            "* **Fallacy of Credibility: Hasty Generalization**: The claim is making a general statement about the outcome of the upcoming election based on a single historical example. This is a hasty generalization, as it is assuming that the outcome of the next election will be similar to the previous one without considering any other factors that might affect the outcome.\n",
            "\n",
            "**Step 4: Conclusion**\n",
            "\n",
            "In conclusion, the text contains a potential fallacy of credibility in the second claim. The claim is making a hasty generalization by assuming that the outcome of the next election will be similar to the previous one without considering other factors.\n"
          ]
        }
      ],
      "source": [
        "# The answer is \"causal oversimplification\" and \"false causality\"\n",
        "print(tot_prompt_1(input_text=\n",
        "             \"\"\"\n",
        "             The last Democrat winner of the New Hampshire primary won the general election.\n",
        "             This year, the winner of the New Hampshire primary will win the general election.\n",
        "             \"\"\",\n",
        "                   pipeline=llama3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LJ2NZdxvMp1"
      },
      "source": [
        "### Version â„–2: Vanilla zero-shot version of ToT prompting with modified output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0dIzb8heaZ5"
      },
      "outputs": [],
      "source": [
        "def tot_prompt_2(input_text, pipeline):\n",
        "    messages = [\n",
        "            {\"role\"   : \"user\",\n",
        "            \"content\":\n",
        "            f\"As a content reviewer, I provide multiple fallacies which might occur in the text; \\\n",
        "            you need to identify the falacies if there're such in the text, else return 'no fallacy'.\\\n",
        "            {fallacies}\\\n",
        "            Text: {input_text}\\\n",
        "            Walk me through this context in manageable parts step by step, \\\n",
        "            summarizing and analyzing as we go. \\\n",
        "            Then, output just the parts of text with their respective identified fallacies on the last line separated by ';'.\\\n",
        "            Answer:\\\n",
        "            \"},\n",
        "    ]\n",
        "\n",
        "    prompt = pipeline.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    terminators = [\n",
        "        pipeline.tokenizer.eos_token_id,\n",
        "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    outputs = pipeline(\n",
        "        prompt,\n",
        "        max_new_tokens=512,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    return outputs[0][\"generated_text\"][len(prompt):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQwH_Uo4uDW_",
        "outputId": "d902ee13-b0b3-420f-c00d-3394fdf1033e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's break down the text and analyze it step by step.\n",
            "\n",
            "**Step 1: Understand the context**\n",
            "The text is discussing the difference between \"smurf\" and \"alt\" in a gaming context. Someone is saying that having an \"alt\" (alternative account) is the same as \"smurfing\" (using a high-ranked account to play with friends).\n",
            "\n",
            "**Step 2: Identify the fallacies**\n",
            "I've identified the following fallacies in the text:\n",
            "\n",
            "* **Appeal to False Authority**: The text mentions someone telling the author that having an \"alt\" is the same as smurfing, but it doesn't provide any evidence or credible source to support this claim.\n",
            "* **False Analogy**: The author is comparing having an \"alt\" to smurfing, but these two concepts are not equivalent. Smurfing is a specific behavior (using a high-ranked account to play with friends), while having an \"alt\" is a different thing (using an alternative account).\n",
            "\n",
            "**Output**\n",
            "Here are the parts of the text with their respective identified fallacies:\n",
            "\n",
            "POST: Someone once told me they have an'alt' cause their main account was too high of rank to play with their friends. It's exactly the same as smurfing; Appeal to False Authority; False Analogy.\n"
          ]
        }
      ],
      "source": [
        "# The answer is \"false analogy\"\n",
        "print(tot_prompt_2(input_text=\n",
        "             \"\"\"\n",
        "             TITLE: There is a difference between a'smurf' and an'alt'.\n",
        "             Please learn it and stop using them interchangeably. POST: Someone once told me\n",
        "             they have an'alt' cause their main account was too high of rank to play with their\n",
        "             friends. It's exactly the same as smurfing.\n",
        "             \"\"\",\n",
        "                   pipeline=llama3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9m3_T0Ch3APR",
        "outputId": "3689e0e0-6cfe-432e-d527-dcd3cbde9e14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Answer:**\n",
            "\n",
            "The text is:\n",
            "\n",
            "America is the best place to live, because it's better than any other country.\n",
            "\n",
            "**Fallacies:**\n",
            "\n",
            "- Ad Populum\n",
            "- Appeal to Positive Emotion\n",
            "\n",
            "**Text with identified fallacies:**\n",
            "\n",
            "America is the best place to live, because it's better than any other country.\n",
            "\n",
            "**Fallacies:**\n",
            "\n",
            "Ad Populum; Appeal to Positive Emotion\n"
          ]
        }
      ],
      "source": [
        "# The answer is \"circular reasoning\"\n",
        "print(tot_prompt_2(input_text=\n",
        "             \"\"\"\n",
        "             America is the best place to live, because it's better than any other country.\n",
        "             \"\"\",\n",
        "                   pipeline=gemma))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqmsd36M3AnQ",
        "outputId": "48690c55-4be6-4956-ee91-e2b975d7f34c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's break down the text step by step.\n",
            "\n",
            "**Step 1:** The text consists of two sentences.\n",
            "\n",
            "1. \"The last Democrat winner of the New Hampshire primary won the general election.\"\n",
            "2. \"This year, the winner of the New Hampshire primary will win the general election.\"\n",
            "\n",
            "**Step 2:** Analyze the first sentence:\n",
            "\n",
            "* The sentence states a fact about the past, which is a neutral statement.\n",
            "* It does not contain any logical fallacies.\n",
            "\n",
            "**Step 3:** Analyze the second sentence:\n",
            "\n",
            "* The sentence makes a claim about the future based on the past.\n",
            "* This is an example of **Appeal to Authority**, as it implies that the past is a reliable indicator of the future. This is a fallacy of credibility.\n",
            "\n",
            "**Step 4:** Combine the two sentences:\n",
            "\n",
            "The text is making a claim about the future based on a past event, and using that past event as a basis for predicting the future.\n",
            "\n",
            "**Output:**\n",
            "\n",
            "The text contains the fallacy of **Appeal to Authority**.\n",
            "\n",
            "Text with fallacies: \"This year, the winner of the New Hampshire primary will win the general election\";\n"
          ]
        }
      ],
      "source": [
        "# The answer is \"causal oversimplification\" and \"false causality\"\n",
        "print(tot_prompt_2(input_text=\n",
        "             \"\"\"\n",
        "             The last Democrat winner of the New Hampshire primary won the general election.\n",
        "             This year, the winner of the New Hampshire primary will win the general election.\n",
        "             \"\"\",\n",
        "                   pipeline=llama3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l5Gy_lx-VPn",
        "outputId": "ea0ddf62-b54c-495d-f543-1d5c8471c590"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Answer:**\n",
            "\n",
            "The text contains the following fallacies:\n",
            "\n",
            "- Appeal to Authority\n",
            "- Appeal to False Authority\n",
            "- Hasty Generalization\n",
            "- Straw Man\n",
            "\n",
            "**Text:**\n",
            "\n",
            "Gatineau police officer tests positive, but masks still not mandatory at checkpoints.\n",
            "\n",
            "Guess what? police need to interact with people at close quarters. They often need to touch or grapple.\n",
            "\n",
            "The masks do little. Cool it. Once they wear masks your anxiety will force you to demand the next step.\n",
            "\n",
            "biohazard suits? get real.\n"
          ]
        }
      ],
      "source": [
        "# The answer is \"slippery slope\"\n",
        "print(tot_prompt_2(input_text=\n",
        "             \"\"\"\n",
        "             TITLE: Gatineau police officer tests positive, but masks still not mandatory at checkpoints POST:\n",
        "             Guess what? police need to interact with people at close quarters. they often need to touch or grapple.\n",
        "             The masks do little. Cool it. Once they wear masks your anxiety will force you to demand the next step.\n",
        "             biohazard suits? get real.\n",
        "             \"\"\",\n",
        "                   pipeline=gemma))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPU8y70p-2Gw",
        "outputId": "01f346d6-71e5-4252-814e-2d90f6da5c7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's break down the text step by step:\n",
            "\n",
            "**POST 1:** \"I wonder how is New York planning on getting out of the lockdown, because when they get out of lockdown probably some cases are going to be going on and the whole infection thing will start again.\"\n",
            "\n",
            "* This post seems to be expressing concern about the effectiveness of the lockdown in New York.\n",
            "* The fallacy present here is: **False Dilemma** (assuming that there are only two options: lockdown or no lockdown, when in reality there might be other alternatives).\n",
            "\n",
            "**POST 2:** \"You can't stop nature. You're going to be infected if you haven't already. We are always trying to fight nature. We just have to let it run its course.\"\n",
            "\n",
            "* This post seems to be arguing that the coronavirus is a natural phenomenon that cannot be stopped, and that trying to fight it is futile.\n",
            "* The fallacies present here are: **Appeal to Nature** (using the concept of \"nature\" to justify a conclusion without providing evidence) and **Appeal to False Authority** (assuming that \"nature\" is an authority that should be listened to, without providing evidence).\n",
            "\n",
            "**Output:**\n",
            "\n",
            "POST 1: False Dilemma; POST 2: Appeal to Nature, Appeal to False Authority\n"
          ]
        }
      ],
      "source": [
        "# The answer is \"appeal to nature\"\n",
        "print(tot_prompt_2(input_text=\n",
        "             \"\"\"\n",
        "             TITLE: Sweden sees higher coronavirus death rate than US after refusing lockdown POST:\n",
        "             I wonder how is New York planning on getting out of the lockdown,\n",
        "             because when they get out of lockdown probably some cases are going to be going\n",
        "             on and the whole infection thing will start again. POST: You cant stop nature.\n",
        "             Youre going to be infected if you havent already. We are always trying to fight nature.\n",
        "             We just have to let it run its course.\n",
        "             \"\"\",\n",
        "                   pipeline=llama3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHb4i_uV_ajw",
        "outputId": "efdb3893-e9dc-47dd-9321-69688cbc3687"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Walk-through:**\n",
            "\n",
            "**Summary:**\n",
            "\n",
            "The text claims that President Putin's sea-ice loss is his gain, arguing that it will provide him with access to shipping routes, energy reserves, and a strategic military advantage.\n",
            "\n",
            "**Analysis:**\n",
            "\n",
            "**Fallacies:**\n",
            "\n",
            "* **Appeal to Positive Emotion:** The text appeals to the reader's positive emotions, such as patriotism and national pride, by highlighting Russia's status as the largest country on the planet.\n",
            "* **Appeal to Nature:** The text appeals to the natural phenomenon of sea-ice loss, claiming that it will benefit Russia's shipping routes and energy reserves.\n",
            "* **Causal Oversimplification:** The text oversimplifies the relationship between sea-ice loss and its impact on shipping routes and energy reserves.\n",
            "\n",
            "**Text with identified fallacies:**\n",
            "\n",
            "The sea-ice loss is President Vladimir Putin's gain; Already the largest country on the planet, Russia stands to gain access to shipping routes and energy reserves, and a strategic military advantage from the opening of the Arctic. Along the Russian coastline, which makes up more than half the Arctic total, winds and currents push old ice away from potential shipping lanes and prevent the build-up of thicker, multi-year ice that would leave other parts of the Arctic impassable for longer periods.\n",
            "\n",
            "**Answer:**\n",
            "\n",
            "The text has Appeal to Positive Emotion, Appeal to Nature, and Causal Oversimplification fallacies.\n"
          ]
        }
      ],
      "source": [
        "# The answer is \"no fallacy\"\n",
        "print(tot_prompt_2(input_text=\n",
        "             \"\"\"\n",
        "             The sea-ice loss is President Vladimir Putin' s gain.\n",
        "             Already the largest country on the planet, Russia stands to gain\n",
        "             access to shipping routes and energy reserves, and a strategic\n",
        "             military advantage from the opening of the Arctic. Along the Russian\n",
        "             coastline, which makes up more than half the Arctic total, winds\n",
        "             and currents push old ice away from potential shipping lanes and\n",
        "             prevent the build-up of thicker, multi-year ice that would leave\n",
        "             other parts of the Arctic impassable for longer periods.\n",
        "             \"\"\",\n",
        "                   pipeline=gemma))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVSl10lG3POo"
      },
      "source": [
        "### Version â„–3: Vanilla few-shot version of ToT prompting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVngsA4I5BLW"
      },
      "outputs": [],
      "source": [
        "def tot_prompt_3(input_text, pipeline):\n",
        "    messages = [\n",
        "            {\"role\"   : \"user\",\n",
        "            \"content\":\n",
        "            f\"As a content reviewer, I provide multiple fallacies and their examples which might occur in the text; \\\n",
        "            you need to identify the falacies if there're such in the text, else return 'no fallacy'.\\\n",
        "            {fallacies_with_examples}\\\n",
        "            Text: {input_text}\\\n",
        "            Walk me through this context in manageable parts step by step, \\\n",
        "            summarizing and analyzing as we go.\\\n",
        "            Then, provide only the parts of text with their respective identified fallacies separated by ';' or the 'no fallacy' label.\\\n",
        "            Answer:\\\n",
        "            \"},\n",
        "    ]\n",
        "    prompt = pipeline.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    terminators = [\n",
        "        pipeline.tokenizer.eos_token_id,\n",
        "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    outputs = pipeline(\n",
        "        prompt,\n",
        "        max_new_tokens=512,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    return outputs[0][\"generated_text\"][len(prompt):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgIKptIG61WQ",
        "outputId": "f8aa59a4-988b-42f4-99bd-bfdc9655dc28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's break down the text into manageable parts and analyze it step by step.\n",
            "\n",
            "**Part 1:** TITLE: There is a difference between a'smurf' and an 'alt'.\n",
            "\n",
            "* No fallacy detected in this part.\n",
            "\n",
            "**Part 2:** POST: Someone once told me they have an 'alt' cause their main account was too high of rank to play with their friends. It's exactly the same as smurfing.\n",
            "\n",
            "* **Appeal to false analogy:** The author is comparing having an 'alt' account to smurfing, which are not the same thing. Smurfing is a practice of creating multiple accounts to play with friends, while having an 'alt' account is a common practice in online gaming where a player creates an alternative account to play with friends who are not in their main account's rank.\n",
            "\n",
            "Here are the parts of the text with their respective identified fallacies separated by ';':\n",
            "\n",
            "* No fallacy; Appeal to false analogy\n"
          ]
        }
      ],
      "source": [
        "# The answer is \"false analogy\"\n",
        "print(tot_prompt_3(input_text=\n",
        "             \"\"\"\n",
        "             TITLE: There is a difference between a'smurf' and an'alt'.\n",
        "             Please learn it and stop using them interchangeably. POST: Someone once told me\n",
        "             they have an'alt' cause their main account was too high of rank to play with their\n",
        "             friends. It's exactly the same as smurfing.\n",
        "             \"\"\",\n",
        "                   pipeline=llama3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeJVAyCH7ClY",
        "outputId": "b64e69ff-45ba-4e19-f5d4-ad1cde792dc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Answer:**\n",
            "\n",
            "America is the best place to live, because it's better than any other country.\n",
            "\n",
            "**Fallacies:**\n",
            "\n",
            "- Appeal to tradition\n",
            "- Straw man\n",
            "- False dilemma\n",
            "\n",
            "**No fallacy:**\n",
            "\n",
            "No fallacy\n"
          ]
        }
      ],
      "source": [
        "# The answer is \"circular reasoning\"\n",
        "print(tot_prompt_3(input_text=\n",
        "             \"\"\"\n",
        "             America is the best place to live, because it's better than any other country.\n",
        "             \"\"\",\n",
        "                   pipeline=gemma))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zw93BICD7N7Y",
        "outputId": "0d820cdb-836c-47f7-977f-73118bfcf58d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's break down the text step by step.\n",
            "\n",
            "**Step 1: Identify the claims**\n",
            "The text makes two claims:\n",
            "\n",
            "1. The last Democrat winner of the New Hampshire primary won the general election.\n",
            "2. This year, the winner of the New Hampshire primary will win the general election.\n",
            "\n",
            "**Step 2: Analyze the claims**\n",
            "The first claim is a historical fact, stating that a previous winner of the New Hampshire primary won the general election. The second claim is a prediction, stating that the winner of the current New Hampshire primary will also win the general election.\n",
            "\n",
            "**Step 3: Identify the fallacy (if any)**\n",
            "The fallacy in this text is:\n",
            "\n",
            "**Hasty generalization**:\n",
            "The second claim is making a generalization based on a single data point (the previous winner). There is no evidence to support the claim that the winner of the current New Hampshire primary will also win the general election. This is a hasty generalization, as the outcome of the current election is uncertain and may not follow the same pattern as the previous one.\n",
            "\n",
            "**Identified fallacy:**\n",
            "\"The last Democrat winner of the New Hampshire primary won the general election.\" This year, the winner of the New Hampshire primary will win the general election;\n"
          ]
        }
      ],
      "source": [
        "# The answer is \"causal oversimplification\" and \"false causality\"\n",
        "print(tot_prompt_3(input_text=\n",
        "             \"\"\"\n",
        "             The last Democrat winner of the New Hampshire primary won the general election.\n",
        "             This year, the winner of the New Hampshire primary will win the general election.\n",
        "             \"\"\",\n",
        "                   pipeline=llama3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF23hzIE-G3H",
        "outputId": "9a0d26cc-4b30-4614-e5a2-90b5fba73a32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Answer:**\n",
            "\n",
            "The text has the following fallacies:\n",
            "\n",
            "Hasty generalization;\n",
            "Causal oversimplification;\n",
            "Fallacy of division;\n",
            "Appeal to anger;\n",
            "False dilemma;\n",
            "Straw man;\n",
            "Appeal to tradition;\n",
            "Tu quoque\n",
            "\n",
            "**No fallacy:**\n",
            "\n",
            "No fallacy found in the text.\n"
          ]
        }
      ],
      "source": [
        "# The answer is \"slippery slope\"\n",
        "print(tot_prompt_3(input_text=\n",
        "             \"\"\"\n",
        "             TITLE: Gatineau police officer tests positive, but masks still not mandatory at checkpoints POST:\n",
        "             Guess what? police need to interact with people at close quarters. they often need to touch or grapple.\n",
        "             The masks do little. Cool it. Once they wear masks your anxiety will force you to demand the next step.\n",
        "             biohazard suits? get real.\n",
        "             \"\"\",\n",
        "                   pipeline=gemma))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPnSn8KZ-hK4",
        "outputId": "ffbf301c-630e-43fb-bb60-e81167e14453"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's break down the text step by step.\n",
            "\n",
            "**Initial Post**\n",
            "I wonder how is New York planning on getting out of the lockdown, because when they get out of lockdown probably some cases are going to be going on and the whole infection thing will start again.\n",
            "\n",
            "* The speaker is wondering about New York's plan to exit the lockdown, and is concerned that cases will continue to arise even after the lockdown is lifted.\n",
            "\n",
            "**Response Post**\n",
            "You can't stop nature. You're going to be infected if you haven't already. We are always trying to fight nature. We just have to let it run its course.\n",
            "\n",
            "* The respondent is arguing that it's futile to try to stop the spread of the virus, and that nature will ultimately prevail. They're suggesting that the best course of action is to let the virus run its course.\n",
            "\n",
            "Now, let's identify any fallacies in the text:\n",
            "\n",
            "* No fallacy in the initial post.\n",
            "* The respondent's statement \"You can't stop nature\" is an example of **Appeal to nature**, which is the idea that nature is always right and that we should accept its course of events without trying to intervene.\n",
            "\n",
            "So, the identified fallacies are:\n",
            "\n",
            "Initial Post: no fallacy\n",
            "Response Post: Appeal to nature\n"
          ]
        }
      ],
      "source": [
        "# The answer is \"appeal to nature\"\n",
        "print(tot_prompt_3(input_text=\n",
        "             \"\"\"\n",
        "             TITLE: Sweden sees higher coronavirus death rate than US after refusing lockdown POST:\n",
        "             I wonder how is New York planning on getting out of the lockdown,\n",
        "             because when they get out of lockdown probably some cases are going to be going\n",
        "             on and the whole infection thing will start again. POST: You cant stop nature.\n",
        "             Youre going to be infected if you havent already. We are always trying to fight nature.\n",
        "             We just have to let it run its course.\n",
        "             \"\"\",\n",
        "                   pipeline=llama3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQUDcsSK_NkE",
        "outputId": "a944d972-007a-405e-c407-eb4830d15355"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Answer:**\n",
            "\n",
            "The text does not contain any fallacies. Therefore, the answer is 'no fallacy'.\n",
            "\n",
            "**Text:**\n",
            "\n",
            "The sea-ice loss is President Vladimir Putin's gain. Already the largest country on the planet, Russia stands to gain access to shipping routes and energy reserves, and a strategic military advantage from the opening of the Arctic. Along the Russian coastline, which makes up more than half the Arctic total, winds and currents push old ice away from potential shipping lanes and prevent the build-up of thicker, multi-year ice that would leave other parts of the Arctic impassable for longer periods.\n",
            "\n",
            "**Fallacies:**\n",
            "\n",
            "No fallacies.\n"
          ]
        }
      ],
      "source": [
        "# The answer is \"no fallacy\"\n",
        "print(tot_prompt_3(input_text=\n",
        "             \"\"\"\n",
        "             The sea-ice loss is President Vladimir Putin' s gain.\n",
        "             Already the largest country on the planet, Russia stands to gain\n",
        "             access to shipping routes and energy reserves, and a strategic\n",
        "             military advantage from the opening of the Arctic. Along the Russian\n",
        "             coastline, which makes up more than half the Arctic total, winds\n",
        "             and currents push old ice away from potential shipping lanes and\n",
        "             prevent the build-up of thicker, multi-year ice that would leave\n",
        "             other parts of the Arctic impassable for longer periods.\n",
        "             \"\"\",\n",
        "                   pipeline=gemma))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fYLXB6bBNn2"
      },
      "source": [
        "### Version â„–4: Advanced zero-shot version of ToT prompting: two-step prompting using reasoning + refining conclusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3RkTNd2BQEk"
      },
      "outputs": [],
      "source": [
        "def tot_prompt_4(input_text, pipeline):\n",
        "    # Step 1: Initial reasoning\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\":\n",
        "            f\"As a content reviewer, I provide multiple fallacies which might occur in the text; \\\n",
        "            you need to identify the fallacies if there're such in the text, else return 'no fallacy'.\\\n",
        "            {fallacies}\\\n",
        "            Text: {input_text}\\\n",
        "            Walk me through this context in manageable parts step by step, \\\n",
        "            summarizing and analyzing as we go. \\\n",
        "            Answer:\\\n",
        "            \"}\n",
        "    ]\n",
        "\n",
        "    # Generate the initial analysis using the pipeline\n",
        "    prompt = pipeline.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    terminators = [\n",
        "        pipeline.tokenizer.eos_token_id,\n",
        "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    outputs = pipeline(\n",
        "        prompt,\n",
        "        max_new_tokens=512,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=True,\n",
        "        temperature=0.1,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    initial_analysis = outputs[0][\"generated_text\"][len(prompt):]\n",
        "\n",
        "    # Step 2: Refining the conclusion\n",
        "    messages_refine = [\n",
        "        {\"role\": \"user\", \"content\":\n",
        "            f\"Based on the analysis provided, distill the findings into a concise summary. \\\n",
        "            For each identified fallacy, specify the text segment and the corresponding fallacy type. \\\n",
        "            Summarize the identified fallacies and their types from the initial analysis:\\\n",
        "            {initial_analysis}\\\n",
        "            Provide only the text where the fallacy was found with their respective fallacies in the format \\\n",
        "            {{text n:fallacy n}},{{text m:fallacy m}}\\\n",
        "            or the 'no fallacy' label if no fallacies in the whole text were detected.\\\n",
        "            Do not output any other text or comments.\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    prompt_refine = pipeline.tokenizer.apply_chat_template(\n",
        "        messages_refine,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    outputs_refine = pipeline(\n",
        "        prompt_refine,\n",
        "        max_new_tokens=512,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=True,\n",
        "        temperature=0.1,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    refined_output = outputs_refine[0][\"generated_text\"][len(prompt_refine):]\n",
        "\n",
        "    return refined_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTuVlCkkF2il",
        "outputId": "c805992d-59c3-478a-9c1a-b588a75e219b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is the concise summary:\n",
            "\n",
            "{Someone once told me...: Appeal to False Authority},{Playing with friends due to high rank, to smurfing: False Analogy},{The reader should learn and stop using the terms interchangeably: Appeal to Emotion}\n"
          ]
        }
      ],
      "source": [
        "# The answer is \"false analogy\"\n",
        "print(tot_prompt_4(input_text=\n",
        "             \"\"\"\n",
        "             TITLE: There is a difference between a'smurf' and an'alt'.\n",
        "             Please learn it and stop using them interchangeably. POST: Someone once told me\n",
        "             they have an'alt' cause their main account was too high of rank to play with their\n",
        "             friends. It's exactly the same as smurfing.\n",
        "             \"\"\",\n",
        "                   pipeline=llama3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WklMVhh1EBiI",
        "outputId": "2c9af835-5156-4907-be9f-311ddb004f72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Summary:**\n",
            "\n",
            "The text contains the fallacies of ad populum, oversimplification, and straw man.\n",
            "\n",
            "**Fallacies:**\n",
            "\n",
            "* **Fallacy of Ad Populum:** \"America is presented as the best place to live based on the assumption that it is better than any other country.\"\n",
            "* **Fallacy of Oversimplification:** \"America is the best place to live\" is an oversimplification of a complex issue.\n",
            "* **Fallacy of Straw Man:** The argument that America is the best place to live assumes that other countries are not as good.\n",
            "\n",
            "**No other fallacies were detected.**\n"
          ]
        }
      ],
      "source": [
        "# The answer is \"circular reasoning\"\n",
        "print(tot_prompt_4(input_text=\n",
        "             \"\"\"\n",
        "             America is the best place to live, because it's better than any other country.\n",
        "             \"\"\",\n",
        "                   pipeline=gemma))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8PTpFXxF3jO"
      },
      "outputs": [],
      "source": [
        "# The answer is \"no fallacy\"\n",
        "print(tot_prompt_4(input_text=\n",
        "             \"\"\"\n",
        "             The sea-ice loss is President Vladimir Putin' s gain.\n",
        "             Already the largest country on the planet, Russia stands to gain\n",
        "             access to shipping routes and energy reserves, and a strategic\n",
        "             military advantage from the opening of the Arctic. Along the Russian\n",
        "             coastline, which makes up more than half the Arctic total, winds\n",
        "             and currents push old ice away from potential shipping lanes and\n",
        "             prevent the build-up of thicker, multi-year ice that would leave\n",
        "             other parts of the Arctic impassable for longer periods.\n",
        "             \"\"\",\n",
        "                   pipeline=llama3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruFPe_3DHOBA"
      },
      "source": [
        "### Version â„–5: Advanced few-shot version of ToT prompting: two-step prompting using reasoning + refining conclusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbQDxP0wHSd9"
      },
      "outputs": [],
      "source": [
        "def tot_prompt_5(input_text, pipeline):\n",
        "    # Step 1: Initial reasoning\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\":\n",
        "            f\"As a content reviewer, I provide multiple fallacies and their examples which might occur in the text; \\\n",
        "            you need to identify the fallacies if there're such in the text, else return 'no fallacy'. Please also return the original text in the end.\\\n",
        "            {fallacies_with_examples}\\\n",
        "            Text: {input_text}\\\n",
        "            Walk me through this context in manageable parts step by step, \\\n",
        "            summarizing and analyzing as we go. \\\n",
        "            Answer:\\\n",
        "            \"}\n",
        "    ]\n",
        "\n",
        "    # Generate the initial analysis using the pipeline\n",
        "    prompt = pipeline.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    terminators = [\n",
        "        pipeline.tokenizer.eos_token_id,\n",
        "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    outputs = pipeline(\n",
        "        prompt,\n",
        "        max_new_tokens=512,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    initial_analysis = outputs[0][\"generated_text\"][len(prompt):]\n",
        "\n",
        "    # Step 2: Refining the conclusion\n",
        "    messages_refine = [\n",
        "        {\"role\": \"user\", \"content\":\n",
        "            f\"For each identified fallacy, specify the text segment and the corresponding fallacy type:\\\n",
        "            {initial_analysis}\\\n",
        "            Provide only the fallacies and copy the piece from the original texts containing these fallacies\\\n",
        "             in the following format (where keys are the fallacies, values are the slices of the original text where the fallacies were detected): \\\n",
        "            {{fallacy type X: text X}}\\\n",
        "            or an empty dictionary if no fallacies in the whole text were detected.\\\n",
        "            Please do not give any comments and do not paraphrase anything.\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    prompt_refine = pipeline.tokenizer.apply_chat_template(\n",
        "        messages_refine,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    outputs_refine = pipeline(\n",
        "        prompt_refine,\n",
        "        max_new_tokens=512,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    refined_output = outputs_refine[0][\"generated_text\"][len(prompt_refine):]\n",
        "\n",
        "    return refined_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMMD7dfeH-sV"
      },
      "outputs": [],
      "source": [
        "# The answer is \"false analogy\"\n",
        "output = tot_prompt_5(input_text=\n",
        "             \"\"\"\n",
        "             TITLE: There is a difference between a'smurf' and an'alt'.\n",
        "             Please learn it and stop using them interchangeably. POST: Someone once told me\n",
        "             they have an'alt' cause their main account was too high of rank to play with their\n",
        "             friends. It's exactly the same as smurfing.\n",
        "             \"\"\",\n",
        "                   pipeline=llama3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "xkbSEscnSgHw",
        "outputId": "f0bcf099-ae5b-44e5-b0b4-57db721ea726"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Here are the fallacies with the corresponding text segments:\\n\\n{False analogy: Someone once told me they have an 'alt' cause their main account was too high of rank to play with their friends. It's exactly the same as smurfing.}\\n{Equivocation: Someone once told me they have an 'alt' cause their main account was too high of rank to play with their friends. It's exactly the same as smurfing.}\\n{Appeal to authority: Someone once told me they have an 'alt' cause their main account was too high of rank to play with their friends. It's exactly the same as smurfing.}\""
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zV-EXG5-IEKq"
      },
      "outputs": [],
      "source": [
        "# The answer is \"circular reasoning\"\n",
        "output2 = tot_prompt_5(input_text=\n",
        "             \"\"\"\n",
        "             America is the best place to live, because it's better than any other country.\n",
        "             \"\"\",\n",
        "                   pipeline=llama3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "JvBSpDIsSiSg",
        "outputId": "22531357-e167-4d87-dc17-09dd2d3fc8b0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{False comparison: \"America is the best place to live, because it\\'s better than any other country.\"}'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FACNXOBKIGoc"
      },
      "outputs": [],
      "source": [
        "# The answer is \"no fallacy\"\n",
        "output3 = tot_prompt_5(input_text=\n",
        "             \"\"\"\n",
        "             The sea-ice loss is President Vladimir Putin' s gain.\n",
        "             Already the largest country on the planet, Russia stands to gain\n",
        "             access to shipping routes and energy reserves, and a strategic\n",
        "             military advantage from the opening of the Arctic. Along the Russian\n",
        "             coastline, which makes up more than half the Arctic total, winds\n",
        "             and currents push old ice away from potential shipping lanes and\n",
        "             prevent the build-up of thicker, multi-year ice that would leave\n",
        "             other parts of the Arctic impassable for longer periods.\n",
        "             \"\"\",\n",
        "                   pipeline=llama3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "fqKNyxj1ShL8",
        "outputId": "688815fa-e3da-4bdd-bda9-2a06f9437140"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{Appeal to authority: \"The sea-ice loss is President Vladimir Putin\\'s gain. Already the largest country on the planet, Russia stands to gain access to shipping routes and energy reserves, and a strategic military advantage from the opening of the Arctic.\"}'"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePrJpcZ8Utjk"
      },
      "source": [
        "## Extract all fallacies for all the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuQzwY4BUsjP"
      },
      "outputs": [],
      "source": [
        "f = open('/content/Test_Dataset.json')\n",
        "data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WW5bp3dYQfN"
      },
      "outputs": [],
      "source": [
        "for n, sample in enumerate(data[100:150]):\n",
        "  print(n)\n",
        "  for k in sample:\n",
        "    output = tot_prompt_5(input_text=k,\n",
        "                          pipeline=llama3)\n",
        "    sample[k] = output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qNNYVRKaNbP"
      },
      "outputs": [],
      "source": [
        "with open('outputs_100_150.json', 'w') as f:\n",
        "    json.dump(data[:50], f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "1EhdDNhdvGFO",
        "6LJ2NZdxvMp1",
        "uVSl10lG3POo"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
