{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from types import MethodType\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from src.classification_models.baseline_models import RandomModel, SilentModel\n",
    "from src.classification_models.openai_based_models import ChatGPTModel\n",
    "from src.classification_models.quantized_llama_based_models import (\n",
    "    LLaMABasedQuantizedModel,\n",
    ")\n",
    "from src.evaluate import eval_dataset\n",
    "from src.experiments_pipelines.pipelines import zero_or_few_shots_pipeline\n",
    "from src.users_study_evaluation import users_study_evaluation\n",
    "from src.utils import setup_logger\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import models\n",
    "\n",
    "from src.classification_models.quantized_llama_based_models import (\n",
    "    LLaMABasedQuantizedModel,\n",
    ")\n",
    "from src.experiments_pipelines.chatbot import ChatBotLLM\n",
    "from src.process_docanno_data import process_data\n",
    "from src.utils import read_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best available device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_predict(self, text):\n",
    "    inputs = self.tokenizer(text, return_tensors='pt')\n",
    "    outputs = self.generate(**inputs)\n",
    "    return self.tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb04eeda6cf49cfacfaef1dc7b21c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "                                  torch_dtype = torch.float16 if device.type in ['cuda', 'mps'] else torch.float32,\n",
    "                                  low_cpu_mem_usage=True,\n",
    "                                  token=HF_TOKEN).to(device)\n",
    "model.tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "model.predict = MethodType(dynamic_predict, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
