{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating spans and converting to evaluation format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_span(paragraph, sentence):\n",
    "    if isinstance(sentence, dict):\n",
    "        if sentence.__contains__('text'):\n",
    "            text = sentence.get('text') \n",
    "            if text in paragraph:\n",
    "                start = paragraph.find(text)\n",
    "                end = paragraph.find(text) + len(text)\n",
    "                return [start, end]\n",
    "            else:\n",
    "                return [0,0]\n",
    "        else:\n",
    "            return [0,0]\n",
    "    elif isinstance(sentence, str):\n",
    "        if sentence in paragraph:\n",
    "            start = paragraph.find(text)\n",
    "            end = paragraph.find(text) + len(text)\n",
    "            return [start, end]\n",
    "        else:\n",
    "            return [0,0]\n",
    "    else:\n",
    "        return [0,0]\n",
    "\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_eval_format(paragraph, prediction):\n",
    "    span = calc_span(paragraph, prediction)\n",
    "    if isinstance(span, list):\n",
    "        return {paragraph: {prediction['class']: span}}\n",
    "    else:\n",
    "        return \"invalid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_eval_format(paragraph, predictions):\n",
    "    span_list = []\n",
    "    for prediction in predictions:\n",
    "        span = calc_span(paragraph, prediction)\n",
    "        span_list.append({prediction['class']: span})\n",
    "    return {paragraph: span_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\abi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\abi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\abi\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\abi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\abi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abi\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../Data/gold_standard_dataset.jsonl', 'r') as file:\n",
    "    data = [json.loads(line) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 145]\n",
      "{'Two of my best friends are really introverted, shy people, and they both have cats. That leads to me believe that most cat lovers are really shy.\\n': {'hasty generalization': [84, 145]}}\n"
     ]
    }
   ],
   "source": [
    "text = data[1]['text']\n",
    "# example prediction from LLM\n",
    "pred = {\"text\": \"That leads to me believe that most cat lovers are really shy.\", \"class\": \"hasty generalization\"}\n",
    "print(calc_span(text, pred))\n",
    "print(convert_to_eval_format(text, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------\n",
    "### Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "input_filename = 'results_mistral_mc.csv'\n",
    "df = pd.read_csv(input_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error decoding JSON: Unterminated string starting at: line 1 column 11 (char 10) in string: [{\"text\": \"It's a mistake. [for]\n",
      "Error decoding JSON: Unterminated string starting at: line 1 column 11 (char 10) in string: [{\"text\": \"It's a mistake. [for]\n",
      "Error decoding JSON: Expecting ',' delimiter: line 1 column 37 (char 36) in string: [{\"text\": \"We don't need to do the \"slippery slope\" thing either.\", \"class\": \"Slippery Slope\"}]\n",
      "Error decoding JSON: Expecting ',' delimiter: line 2 column 62 (char 169) in string: [{\"text\": \"There are many of us and we can't all be wrong.\", \"class\": \"Appeal to Popularity (Ad Populum)\"},\n",
      " {\"text\": \"You are doing a dis-service to your country being\"politically correct\" in regard to Islam and your children will suffer because of it.\", \"class\": \"Guilt by Association\"}]\n",
      "Error decoding JSON: Expecting ',' delimiter: line 2 column 239 (char 349) in string: [{\"text\": \"So he's saying liberals are better educated than conservatives. We agree.\", \"class\": \"No fallacy\"},\n",
      " {\"text\": \"Or, more accurately, many liberal professors abuse their positions by attempting to force their political opinions into subjects they don't belong in, like the English professor who insists their students each write a paper on\"Why the GOP are terrorists\" and grade students poorly who disagree with that opinion (experienced that first hand).\", \"class\": \"Ad Hominem: Abusive\"},\n",
      " {\"text\": \"Many of the big name institutions have become incestuous single minded orgies which no longer challenge students to think out of the box, challenge assumed truths and seek to learn all they can, but instead have become factories to produce a very specific set of opinions and mindsets never to be questioned.\", \"class\": \"Straw Man\"}]\n",
      "Error decoding JSON: Unterminated string starting at: line 1 column 11 (char 10) in string: [{\"text\": \"Check [fightthenewdrug.org]\n",
      "Error decoding JSON: Expecting ',' delimiter: line 1 column 96 (char 95) in string: [{\"text\": \"The output for the identification of the fallacy should be formatted as follows: [{\"text\":\", \"\"class\":\"\"}]\n",
      "Error decoding JSON: Expecting ',' delimiter: line 1 column 70 (char 69) in string: [{\"text\": \"Seems about right. Fines are punishments. Calling a fine \"punitive\" is like calling a textbook \"educational\". The gov't has been going all out to educate people. Anyone who still doesn't know the rules by now just isn't bothering to educate themselves.\", \"class\": \"no fallacy\"},\n",
      " {\"text\": \"Yeah, this doctor is a moron. Not smart like you.\", \"class\": \"Ad Hominem: Abusive\"}]\n",
      "Error decoding JSON: Expecting ',' delimiter: line 2 column 96 (char 202) in string: [{\"text\": \"6 is a bit too young, especially a game about killing people\", \"class\": \"Appeal to Tradition\"},\n",
      " {\"text\": \"The output for the identification of the fallacy should be formatted as follows: [{\"text\":\", \"class\": \"}]\n",
      "Error decoding JSON: Expecting property name enclosed in double quotes: line 1 column 28 (char 27) in string: [{\"text\":\"\", \"class\": \"}\", {\"text\":\"\", \"class\": \"}]\n",
      "Error decoding JSON: Invalid \\escape: line 1 column 67 (char 66) in string: [{\"text\": \"No. They were being hunted. Please take this down u/the\\_ebb\\_and\\_flow\", \"class\": \"Ad Hominem: Tu Quoque\"}]\n",
      "Error decoding JSON: Unterminated string starting at: line 1 column 11 (char 10) in string: [{\"text\": \"Please actually look at the divorce/marriages statistics first:[Here's more statistics on Divorce!]\n",
      "Error decoding JSON: Expecting ',' delimiter: line 2 column 178 (char 328) in string: [{\"text\": \"Because contrary to what the major political parties and media want you to believe, not everyone fits into boxes.\", \"class\": \"no fallacy\"},\n",
      " {\"text\": \"Everyone fits into boxes, just sometimes they're boxes with only one person in.\", \"class\": \"Ad Hoc Fallacy (also known as Appeal to Simplicity or Avoiding the Issue)\":\n",
      " This fallacy is not directly mentioned in the text but the statement \"It's extremely individualized\" can be seen as an attempt to simplify complex issues by reducing them to individual cases, which is a form of Ad Hoc Fallacy. However, since the text does not contain any clear examples of other fallacies, no fallacy should be assigned to it with certainty.}]\n"
     ]
    }
   ],
   "source": [
    "def extract_json_string(model_output):\n",
    "    \"\"\"\n",
    "    Extracts valid JSON strings from the text within square brackets [].\n",
    "    Only includes JSON objects that have a non-empty 'class' field.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find all substrings within square brackets\n",
    "        json_strings = re.findall(r'\\[.*?\\]', model_output, re.DOTALL)\n",
    "        \n",
    "        valid_json_objects = []\n",
    "        \n",
    "        for json_str in json_strings:\n",
    "            try:\n",
    "                # Clean the JSON string to ensure it's in valid JSON format\n",
    "                json_str = json_str.replace('\\\\\"', '\\\"').replace(\"\\\\'\", \"'\")\n",
    "\n",
    "                json_objects = json.loads(json_str)\n",
    "                \n",
    "                for obj in json_objects:\n",
    "                    if obj.get(\"class\"):\n",
    "                        valid_json_objects.append(obj)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e} in string: {json_str}\")\n",
    "                continue\n",
    "        \n",
    "        # Convert the list of valid JSON objects back to a JSON string\n",
    "        if valid_json_objects:\n",
    "            return json.dumps(valid_json_objects)\n",
    "        \n",
    "        return \"[]\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting JSON: {e}\")\n",
    "        return \"[]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['json_string'] = df['output'].apply(extract_json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fallacy_class(model_output):\n",
    "    \"\"\"\n",
    "    Extracts fallacy class from the model_output string.\n",
    "    If the class is \"no fallacy\" or \"No fallacy\", returns an empty JSON string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find all substrings within curly braces {}\n",
    "        matches = re.findall(r'\\{(.*?)\\}', model_output)\n",
    "        \n",
    "        # Filter out \"no fallacy\" or \"No fallacy\" classes\n",
    "        valid_classes = [match for match in matches if match.lower() != \"no fallacy\"]\n",
    "        \n",
    "        # Create a list of dictionaries with the extracted classes\n",
    "        if valid_classes:\n",
    "            result = [{\"class\": cls} for cls in valid_classes]\n",
    "            return json.dumps(result)\n",
    "        \n",
    "        return \"[]\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting fallacy class: {e}\")\n",
    "        return \"[]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['json_string'] = df['output'].apply(extract_fallacy_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('results_mistral_mc_eval.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert .csv to eval .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert csv results to eval json\n",
    "import csv\n",
    "\n",
    "# Read CSV file\n",
    "file_path = \"results_mistral_mc_eval.csv\"\n",
    "\n",
    "file = open(file_path, mode = 'r', newline = '', encoding='utf8')\n",
    "csv_reader = csv.reader(file, delimiter=',')\n",
    "\n",
    "#skip header\n",
    "next(csv_reader)\n",
    "# Read the rest of the rows\n",
    "data = [row for row in csv_reader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input = [row[0] for row in data]\n",
    "predictions = [json.loads(row[2]) for row in data]\n",
    "eval_data = []\n",
    "for n, paragraph in enumerate(input):\n",
    "    eval_data.append(convert_list_to_eval_format(paragraph, predictions[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = file_path.replace('.csv', '.json')\n",
    "with open(f'../Data/formatted_results/eval_{file_name}', 'w', encoding='utf8') as out:\n",
    "    json.dump(eval_data , out, indent = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "input = [row[0] for row in data]\n",
    "predictions = [json.loads(row[2]) for row in data]\n",
    "eval_data = []\n",
    "for n, paragraph in enumerate(input):\n",
    "    prediction = predictions[n]\n",
    "    if prediction:\n",
    "        eval_data.append({\"text\": paragraph, \"class\": prediction[0]['class']})\n",
    "    else:\n",
    "        eval_data.append({\"text\": paragraph, \"class\": 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = file_path.replace('.csv', '.json')\n",
    "with open(f'../Data/formatted_results/gkp_eval_{file_name}', 'w', encoding='utf8') as out:\n",
    "    for line in eval_data:\n",
    "        json_string = json.dumps(line)\n",
    "        out.write(json_string + '\\n')\n",
    "    # json.dump(eval_data , out, indent = 0)\n",
    "    # out.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project-hJMgCWV1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
